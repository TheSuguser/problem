{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../\"\n",
    "sys.path.append(_snlp_book_dir) \n",
    "import statnlpbook.lm as lm\n",
    "import statnlpbook.ohhla as ohhla\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load ../../../..//data/ohhla/train/www.ohhla.com/anonymous/nas/distant/tribal.nas.txt.html\n"
     ]
    }
   ],
   "source": [
    "_snlp_train_dir = _snlp_book_dir + \"/data/ohhla/train\"\n",
    "_snlp_dev_dir = _snlp_book_dir + \"/data/ohhla/dev\"\n",
    "_snlp_train_song_words = ohhla.words(ohhla.load_all_songs(_snlp_train_dir))\n",
    "_snlp_dev_song_words = ohhla.words(ohhla.load_all_songs(_snlp_dev_dir))\n",
    "assert(len(_snlp_train_song_words)==1041496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_NGramLM(lm.CountLM):\n",
    "    def __init__(self, train, order):\n",
    "        \"\"\"\n",
    "        Create an NGram language model.\n",
    "        Args:\n",
    "            train: list of training tokens.\n",
    "            order: order of the LM.\n",
    "        \"\"\"\n",
    "        super().__init__(set(train), order)\n",
    "        self._counts = collections.defaultdict(float)\n",
    "        self._norm = collections.defaultdict(float)\n",
    "        self._counts_history = collections.defaultdict(float)\n",
    "        self._counts_word = collections.defaultdict(float)\n",
    "        for i in range(self.order, len(train)):\n",
    "            history = tuple(train[i - self.order + 1 : i])\n",
    "            word = train[i]\n",
    "            self._counts[(word,) + history] += 1.0\n",
    "            self._norm[history] += 1.0\n",
    "        for j in list(self._counts.keys()):\n",
    "            \n",
    "            self._counts_word[list(j)[0]] += 1.0\n",
    "        for k in list(self._counts.keys()):\n",
    "            self._counts_history[list(k)[1]] += 1.0\n",
    "    def counts(self, word_and_history):\n",
    "        #print(self._counts)\n",
    "        #print(word_and_history, self._counts[word_and_history])\n",
    "        return self._counts[word_and_history]\n",
    "    def norm(self, history):\n",
    "        #print(history, self._norm[history])\n",
    "        return self._norm[history]\n",
    "    def counts_history(self, history):\n",
    "        return self._counts_history[history]\n",
    "    def counts_word(self, word):\n",
    "        return self._counts_word[word]\n",
    "    def counts_all(self):\n",
    "        return len(list(self._counts.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KneseNeyLM(lm.LanguageModel):\n",
    "    def __init__(self, main, backoff, discount):\n",
    "        super().__init__(main.vocab, main.order)\n",
    "        self.main = main\n",
    "        self.backoff = backoff\n",
    "        self.discount = discount\n",
    "    def probability(self, word, *history):\n",
    "        sub_history = tuple(history[-1:])\n",
    "        word_and_history = (word,) + sub_history\n",
    "        #print(word_and_history)\n",
    "        history = \"\".join(sub_history)\n",
    "        \n",
    "        #if self.main.counts_history(history) == 0.0:\n",
    "        if self.backoff.counts((history,)) == 0:\n",
    "            \n",
    "            return self.backoff.probability(word)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            p1 = max((self.main.counts(word_and_history) - self.discount),0)/ self.backoff.counts((history,))\n",
    "            lmb = self.discount / self.backoff.counts((history,)) * self.main.counts_history(history)\n",
    "            pc = self.main.counts_word(word) / self.main.counts_all()\n",
    "\n",
    "            #print(p1+lmb*pc)\n",
    "\n",
    "            return p1 + lmb * pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_train = lm.inject_OOVs(_snlp_train_song_words + _snlp_dev_song_words)\n",
    "bigram = new_NGramLM(oov_train, 2)\n",
    "unigram = lm.NGramLM(oov_train,1)\n",
    "my_lm =KneseNeyLM(bigram, unigram, 75)\n",
    "oov_vocab = set(oov_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lm(vocab):\n",
    "    \"\"\"\n",
    "    Return an instance of `lm.LanguageModel` defined over the given vocabulary.\n",
    "    Args:\n",
    "        vocab: the vocabulary the LM should be defined over. It is the union of the training and test words.\n",
    "    Returns:\n",
    "        a language model, instance of `lm.LanguageModel`.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return lm.OOVAwareLM(my_lm, vocab - oov_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 3\n",
    "_snlp_test_dir = _snlp_book_dir + \"/data/ohhla/dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! SETUP 4\n",
    "_snlp_test_song_words = ohhla.words(ohhla.load_all_songs(_snlp_test_dir))\n",
    "_snlp_test_vocab = set(_snlp_test_song_words)\n",
    "_snlp_dev_vocab = set(_snlp_dev_song_words)\n",
    "_snlp_train_vocab = set(_snlp_train_song_words)\n",
    "_snlp_vocab = _snlp_test_vocab | _snlp_train_vocab | _snlp_dev_vocab\n",
    "_snlp_lm = create_lm(_snlp_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Sum: 0.9999675691209431, ~1: False, <=1: True\n",
      "1000\n",
      "Sum: 11.937828439739745, ~1: False, <=1: False\n",
      "10000\n",
      "Sum: 25.809477620261173, ~1: False, <=1: False\n"
     ]
    }
   ],
   "source": [
    "#! ASSESSMENT 1\n",
    "_snlp_test_token_indices = [100, 1000, 10000]\n",
    "_eps = 0.000001\n",
    "for i in _snlp_test_token_indices:\n",
    "    print(i)\n",
    "    result = sum([_snlp_lm.probability(word, *_snlp_test_song_words[i-_snlp_lm.order+1:i]) for word in _snlp_vocab])\n",
    "    print(\"Sum: {sum}, ~1: {approx_1}, <=1: {leq_1}\".format(sum=result, \n",
    "                                                            approx_1=abs(result - 1.0) < _eps, \n",
    "                                                            leq_1=result - _eps <= 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.32932582901255"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.perplexity(_snlp_lm, _snlp_test_song_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
